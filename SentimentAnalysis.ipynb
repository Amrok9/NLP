{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Load or Simulate Text Data**\n",
        "For simplicity, we’ll simulate some labeled review data. You can replace this with any CSV."
      ],
      "metadata": {
        "id": "lgOmLOpTB2T5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Simulate Sentiment-Labeled Text Data"
      ],
      "metadata": {
        "id": "LypRv1syHufH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "Ed5Y-ZPDBMAp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'text': [\n",
        "        \"I love this product!\",\n",
        "        \"Worst purchase ever.\",\n",
        "        \"Totally satisfied with the service.\",\n",
        "        \"Horrible experience.\",\n",
        "        \"Amazing quality, will buy again!\",\n",
        "        \"Not worth the price.\"\n",
        "    ],\n",
        "    'label': [1, 0, 1, 0, 1, 0]\n",
        "}\n",
        "df = pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Tokenize Text with BERT"
      ],
      "metadata": {
        "id": "ijcGF6avHyeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "encodings = tokenizer(df['text'].tolist(), truncation=True, padding=True, return_tensors='pt')\n",
        "labels = df['label'].values"
      ],
      "metadata": {
        "id": "QAbnTyPkBO5H"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Create a PyTorch Dataset"
      ],
      "metadata": {
        "id": "p0IcPt6sH1sn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "dataset = SentimentDataset(encodings, labels)"
      ],
      "metadata": {
        "id": "RVX0WPZzBtHp"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Load BERT Model for Classification"
      ],
      "metadata": {
        "id": "czvKUesDH6NH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7E9eSfXzBe9Z",
        "outputId": "f79c1313-31ba-4dff-ebd9-53f9a3278609"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅  Train the Model"
      ],
      "metadata": {
        "id": "omNGA7btH9bG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=2,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=1,\n",
        "    save_strategy='no',\n",
        "    disable_tqdm=False\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xvvUAj5RCYfs",
        "outputId": "b7dea010-6855-480f-aab8-ed2297e9019e"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [30/30 00:38, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.760000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.908600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.681000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.754600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.655300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.768100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.742300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.530700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.550100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.580500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.419700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.523700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.419000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.378600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.437500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.350200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.347900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.287100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.336900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.200300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.227200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.317900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.203900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.170700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.136500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.144500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.136200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.119400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.113000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.108400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=30, training_loss=0.4103278748691082, metrics={'train_runtime': 42.5772, 'train_samples_per_second': 1.409, 'train_steps_per_second': 0.705, 'total_flos': 277499941200.0, 'train_loss': 0.4103278748691082, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Predict Sentiment on New Text"
      ],
      "metadata": {
        "id": "go_wJ3f0IDbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = [\"This is excellent!\", \"Terrible service...\"]\n",
        "\n",
        "# Tokenize the test text\n",
        "test_inputs = tokenizer(test_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Get model predictions\n",
        "with torch.no_grad():\n",
        "    outputs = model(**test_inputs)\n",
        "    predictions = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "# Print the sentiment results\n",
        "for text, pred in zip(test_text, predictions):\n",
        "    sentiment = \"Positive\" if pred.item() == 1 else \"Negative\"\n",
        "    print(f\"Text: {text} → Sentiment: {sentiment}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2zVi9fjCiU-",
        "outputId": "b65535d3-0fb2-4ac7-cd33-c00800c0c8b6"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: This is excellent! → Sentiment: Positive\n",
            "Text: Terrible service... → Sentiment: Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📝 Project Summary:\n",
        "\n",
        "This project uses a pre-trained BERT model (`bert-base-uncased`) for binary sentiment classification. The model is fine-tuned on a small custom dataset of labeled reviews (0 = Negative, 1 = Positive).\n",
        "\n",
        "**Steps:**\n",
        "1. Simulate a labeled text dataset using `pandas`.\n",
        "2. Tokenize text using `BertTokenizer`.\n",
        "3. Create a custom PyTorch `Dataset`.\n",
        "4. Load `BertForSequenceClassification` with `num_labels=2`.\n",
        "5. Train the model using Hugging Face's `Trainer` and `TrainingArguments`.\n",
        "6. Predict sentiment on new examples using the fine-tuned model.\n",
        "\n",
        "**Important Notes:**\n",
        "- The warning about uninitialized weights is expected: the classifier head is randomly initialized and learned during training.\n",
        "- To disable `wandb` logging prompts, use:  \n",
        "  `os.environ[\"WANDB_DISABLED\"] = \"true\"`"
      ],
      "metadata": {
        "id": "y4yYXDtvJOWo"
      }
    }
  ]
}