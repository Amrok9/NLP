{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Text Summarizer with T5**\n",
        "**Goal:** Use the pretrained T5 model to summarize large chunks of text by framing summarization as a text-to-text task."
      ],
      "metadata": {
        "id": "P2uiIms01TJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load or Input Long Text**"
      ],
      "metadata": {
        "id": "atuWsAm01cfK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9ax_4ZhF1NfB"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "Artificial intelligence (AI) has rapidly transformed many industries, including healthcare, finance, and education.\n",
        "With the rise of machine learning and deep learning, AI systems are now capable of analyzing large datasets,\n",
        "making accurate predictions, and even generating human-like text and images. As technology continues to advance,\n",
        "researchers are focused on developing more transparent, ethical, and general-purpose AI that can safely assist humans\n",
        "in a wide range of complex tasks.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the T5 Model and Tokenizer**"
      ],
      "metadata": {
        "id": "nP5T3NNw1lUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "# Load the T5 tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
      ],
      "metadata": {
        "id": "Pe6AeT9h1h2F"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"summarize: \" + text\n",
        "\n",
        "# Tokenize the input text into input IDs (for the model)\n",
        "input_ids = tokenizer.encode(\n",
        "    input_text,\n",
        "    return_tensors=\"pt\",  # Return as PyTorch tensor\n",
        "    max_length=512,       # Truncate if the input is too long\n",
        "    truncation=True       # Enable truncation\n",
        ")"
      ],
      "metadata": {
        "id": "VvtUMmQD1mYG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Generate the Summary\n",
        "\n",
        "# Generate summary token IDs from the input\n",
        "summary_ids = model.generate(\n",
        "    input_ids,\n",
        "    max_length=100,    # Maximum length of the generated summary\n",
        "    min_length=20,     # Minimum length to avoid too-short summaries\n",
        "    length_penalty=2.0,  # Penalize longer summaries (can be tuned)\n",
        "    num_beams=4,         # Beam search to improve quality\n",
        "    early_stopping=True  # Stop once best summary is found\n",
        ")\n",
        "\n",
        "# Decode the generated token IDs into a string\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the final summary\n",
        "print(\"Summary:\\n\", summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbFPjWuf1y_V",
        "outputId": "925938d5-b735-4209-f69d-c3c26a7cd035"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            " AI systems are now capable of analyzing large datasets, making accurate predictions, and even generating human-like text and images. as technology advances, researchers are focused on developing more transparent, ethical, and general-purpose AI that can safely assist humans in a wide range of complex tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìù Project 32: Text Summarizer with T5\n",
        "\n",
        "This project uses the `t5-small` model to summarize long text.\n",
        "\n",
        "- T5 treats summarization as a text-to-text task.\n",
        "- Input: `\"summarize: \" + text`\n",
        "- Output: A short, clear summary.\n",
        "\n",
        "**Steps:**\n",
        "1. Load T5 model & tokenizer\n",
        "2. Tokenize input with prefix\n",
        "3. Generate and decode summary\n",
        "\n",
        "Model: `t5-small` | Library: Transformers\n"
      ],
      "metadata": {
        "id": "Da2tDHqxDQkn"
      }
    }
  ]
}